# Eedi - Mining Misconceptions in Mathematics (2024)
> Last updated: 2026-01-23
> Source count: 1
---

### Eedi - Mining Misconceptions in Mathematics (2024)

**竞赛背景：**
- **主办方**：The Learning Agency (TLA)
- **目标**：从数学问题中识别学生的误解
- **应用场景**：教育科技、个性化学习、智能辅导系统
- **社会意义**：自动化误解检测，帮助教师针对性教学

**任务描述：**
从数学问题文本中识别最相关的误解（Misconception）：
- **输入**：数学问题文本 + 4 个选项（1 个正确，3 个错误）
- **输出**：Top 3 最相关的误解类别（2,587 种类型）
- **评估**：MAP@3 (Mean Average Precision at 3)

**数据集规模：**
- 训练集：1,868 个数学问题
- 误解类别：2,587 种类型
- 数据来源：Vanderbilt 专家标注

**数据特点：**
1. **多标签问题**：一个问题可能有多个相关的误解
2. **解释依赖**：需要理解问题的推理过程
3. **领域知识**：需要深入的数学专业知识

**评估指标：**
- **MAP@3**：预测的前 3 个误解的平均精度
- 需要对误解类别进行排序

**竞赛约束：**
- 奖金池：$12,000
- 时间限制：约 2 个月

**最终排名：**
- 1st Place: Team MTH 101 (Raja Biswas) - Score ~0.637
- 2nd Place: -
- 3rd Place: -

**技术趋势：**
- **检索增强生成 (RAG)**：检索相似问题 + LLM 生成答案
- **多阶段流水线**：检索 + 重排的分离架构
- **LLM 微调**：Qwen 系列 LLM 用于教育任务

**关键创新：**
- **多阶段检索+重排流水线** (1st Place)
- **Distractor prediction** (1st Place)：预测错误答案与误解的亲和度
- **Retrieval-augmented approach** (1st Place)：嵌入模型检索候选误解

---

### Eedi - Mining Misconceptions in Mathematics (2024) - 2025-01-22
**Source:** [Kaggle Competition](https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics) | [Lessons Learned](https://the-learning-agency.com/the-cutting-ed/article/lessons-learned-from-hosting-ai-competitions-in-edtech/)
**Category:** NLP/LLM (教育 AI / 误解检测)
**Key Techniques:**
- **多阶段检索+重排流水线**: Qwen LLMs 用于初始检索和重排序
- **Distractor prediction**: 预测错误答案与误解的亲和度
- **Retrieval-augmented approach**: 嵌入模型检索候选误解
- **Same winner as MAP**: Team MTH 101 (Raja Biswas) 赢得了 Eedi 和 MAP

**Results:** 1st Place score ~0.637, $12,000 奖金, 数据集 1,868 个数学问题

#### 前排方案详细技术分析

**1st Place - Team MTH 101 (Raja Biswas)**

核心技巧：
- **多阶段检索+重排流水线**：Qwen LLMs 用于初始检索和重排序
- **Distractor prediction**：预测错误答案与误解的亲和度
- **Retrieval-augmented approach**：嵌入模型检索候选误解
- **LLM 微调**：Qwen 系列 LLM 在教育数据上微调
- **集成融合**：多个模型的加权组合

实现细节：
- 检索阶段：使用嵌入模型检索相似历史问题和误解
- 重排序：Qwen LLM 对检索结果进行精排
- Distractor prediction：单独的模型预测错误选项的迷惑性
- 最终 MAP@3：~0.637，获得 $12,000 奖金

**与 MAP 的关系**：
- 同一冠军团队（Team MTH 101）
- 技术框架一脉相承：检索 + 推理 + 集成
- MAP 是 Eedi 的扩展版本，处理更复杂的学生回答数据

**2nd Place - Kazuhito Yonekawa et al.**

核心技巧：
- **多阶段 retrieve-and-rank**：嵌入检索 + LLM 重排
- **Qwen2.5-72B 主模型**：大规模 LLM 用于推理和重排
- **CoT 提示工程**：思维链提示引导模型推理
- **后处理优化**：基于误解层次结构的后处理

实现细节：
- Qwen2.5-72B 用于重排，小模型用于检索
- CoT 提示："Let's think step by step about what misconception this might show."
- 后处理：父子误解关系的层次约束
- 最终 MAP@3：~0.636

**3rd Place - waseda-pochi**

核心技巧：
- **Magic boost post-processing**：针对特定误解类型的 boost
- **Unknown misconception correction**：修正"未知"误解的预测
- **Qwen2.5-32B 模型**：平衡性能和效率
- **特征工程**：问题难度、选项分布等特征

实现细节：
- Magic boost：为低召回但高精度误解提升权重
- Unknown correction：使用相似误解替换"Unknown"标签
- 特征：问题长度、选项数量、数字密度等
- 最终 MAP@3：~0.635

**4th Place - (匿名团队)**

核心技巧：
- **CoT features 辅助**：思维链特征作为额外输入
- **分组合成数据**：按问题类型分组生成合成数据
- **Qwen2.5-32B 集成**：多个模型集成
- **两阶段训练**：预训练 + 微调

实现细节：
- CoT features：提取推理链中的关键步骤作为特征
- 分组合成：按代数、几何、概率等分组生成合成问题
- 两阶段：在通用数学数据上预训练，Eedi 数据微调
- 最终 MAP@3：~0.634

**5th Place - ebi-ktr**

核心技巧：
- **Bi-encoder 检索**：双编码器架构高效检索
- **Listwise reranking**：列表级重排代替点级
- **多模型融合**：嵌入模型 + LLM 融合
- **负采样策略**：困难负样本挖掘

实现细节：
- Bi-encoder：Question 和 Misconception 分别编码
- Listwise：LambdaLoss 优化整个排序列表
- 负采样：选择与问题相似但不是正确误解的样本
- 最终 MAP@3：~0.633

**6th Place - (匿名团队)**

核心技巧：
- **QLoRA 微调**：参数高效微调大模型
- **Qwen2.5-14B 架构**：较小模型降低成本
- **集成策略**：多个 LoRA 适配器集成
- **数据增强**：数学问题改写增强

实现细节：
- QLoRA：rank=64, α=16, dropout=0.05
- LoRA 适配器：在 Qwen2.5-14B 上训练 4-6 个适配器
- 数据增强：改写问题表述，保持误解类型不变
- 最终 MAP@3：~0.632

**7th (Private) / 2nd (Public) - terekaerumasahmet**

核心技巧：
- **Multi-loss 组合**：多种损失函数组合
- **Soft labels 蒸馏**：从大模型蒸馏软标签
- **Qwen2.5-32B 主模型**：平衡性能
- **多种采样策略**：Top-k, Nucleus, Temperature sampling

实现细节：
- Multi-loss：BCE + Focal + Label Smoothing 组合
- Soft labels：从 72B 教师模型蒸馏，温度 T=2
- 采样策略：推理时结合多种采样方法
- 最终 MAP@3：~0.631 (Private), ~0.64 (Public)

**8th Place - (匿名团队)**

核心技巧：
- **多阶段检索系统**：粗检索 + 精检索两级架构
- **Listwise reranking**：列表级排序优化
- **Qwen2.5-32B 系列**：多个变体模型集成
- **特征融合**：语义特征 + 统计特征融合

实现细节：
- 两级检索：第一级 BM25，第二级向量检索
- Listwise：ListMLE 损失优化排序列表
- 特征融合：TF-IDF + Embedding + 统计特征
- 最终 MAP@3：~0.630

**9th (Private) / 7th (Public) - (匿名团队)**

核心技巧：
- **QLoRA 微调**：参数高效微调
- **多任务学习**：同时预测误解和选项正确性
- **Qwen2.5-14B 架构**：效率优先
- **集成学习**：多个微调模型集成

实现细节：
- QLoRA：在嵌入层和注意力层添加 LoRA
- 多任务：主任务误解预测，辅助任务选项正确性
- 集成：5-7 个不同随机种子的 QLoRA 模型
- 最终 MAP@3：~0.629 (Private), ~0.631 (Public)

**10th Place - (匿名团队)**

核心技巧：
- **合成数据生成**：LLM 生成额外训练数据
- **知识蒸馏**：20B → 8B 模型蒸馏
- **Qwen2.5-32B 教师 → Qwen2.5-8B 学生**：4:1 压缩
- **集成融合**：教师 + 学生模型集成

实现细节：
- 合成数据：GPT-4 生成相似问题和误解配对
- 蒸馏：教师软标签 + 学生硬标签联合训练
- 集成：教师权重 0.7，学生权重 0.3
- 最终 MAP@3：~0.628

---

